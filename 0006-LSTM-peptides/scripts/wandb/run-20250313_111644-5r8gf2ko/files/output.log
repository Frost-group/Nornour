Using configuration: {'epochs': 2, 'batch_size': 16, 'learning_rate': 0.00063, 'layers': 2, 'output_size': 21, 'hidden_size': 256, 'dropout': 0.7}
longest peptide: 30 AA
Size of peptides for model: 15
Total peptides: 14220
Training set size: 12087
Test set size: 2133
Max index: 20, Vocab size: 21

-------Starting Training-------
_________________________________
Epoch [1/2]: 100%|███████████████████████████████████████████████████████| 755/755 [00:05<00:00, 149.31it/s, accuracy=tensor(0.4330), loss=1.94]
Epoch [1/2], Loss: 2.249494645927126, Accuracy: 0.3194477295441343
Epoch [2/2]: 100%|███████████████████████████████████████████████████████| 755/755 [00:04<00:00, 151.95it/s, accuracy=tensor(0.3795), loss=2.03]
Epoch [2/2], Loss: 2.01213696808215, Accuracy: 0.3909472566961453
-------End of Training--------
-----------------------------

------Starting Testing------
-----------------------------
Test logs:
 Accuracy: 39.80%, Avg loss: 2.0030

------End of testing--------
-----------------------------
Model saved to ../models/lstm_peptides_model.pt
Directory created: ../data/gen_13-03-2025/generated_peptides_2
Generation command:

python LSTM-peptides-generation.py --dataset_path ../../0003d-DBAASP-Database/Database_of_Antimicrobial_Activity_and_structure_of_Peptides --model_path ../lstm_peptides_model.pt --output_path d --output_size 21 --epochs 2 --batch_size 16 --learning_rate 0.00063 --hidden_size 256 --layers 2 --dropout 0.7 --temperature 1.0 --num_sequences 40000 --min_length 1 --max_length 15 --seed r
Traceback (most recent call last):
  File "/Users/gonterigor/Documents/Imperial/Research/Nornour/0006-LSTM-peptides/scripts/LSTM-peptide-model.py", line 512, in <module>
    seed_input = input('Seed (str), r for random: ')
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
