Using configuration: {'epochs': 20, 'batch_size': 16, 'learning_rate': 1e-05, 'layers': 4, 'output_size': 22, 'hidden_size': 256, 'dropout': 0.2}
longest peptide: 30 AA
Size of peptides for model: 30
Total peptides: 14220
Training set size: 12087
Test set size: 2133
Max index: 21, Vocab size: 22

-------Starting Training-------
_________________________________
Epoch [1/20]: 100%|████████████████████████████████████████| 755/755 [00:54<00:00, 13.85it/s, accuracy=tensor(0.3879), loss=2.23]
Epoch [1/20], Loss: 2.1871290791113647, Accuracy: 0.48501062393188477
Epoch [2/20]: 100%|████████████████████████████████████████| 755/755 [00:43<00:00, 17.52it/s, accuracy=tensor(0.5431), loss=1.64]
Epoch [2/20], Loss: 1.7722347335310171, Accuracy: 0.49651432037353516
Epoch [3/20]: 100%|████████████████████████████████████████| 755/755 [00:49<00:00, 15.20it/s, accuracy=tensor(0.4935), loss=1.78]
Epoch [3/20], Loss: 1.68345630026811, Accuracy: 0.5244688987731934
Epoch [4/20]: 100%|████████████████████████████████████████| 755/755 [00:46<00:00, 16.35it/s, accuracy=tensor(0.5905), loss=1.41]
Epoch [4/20], Loss: 1.6247814737408366, Accuracy: 0.5301153659820557
Epoch [5/20]: 100%|████████████████████████████████████████| 755/755 [00:45<00:00, 16.66it/s, accuracy=tensor(0.5690), loss=1.48]
Epoch [5/20], Loss: 1.5808513088731577, Accuracy: 0.5351934432983398
Epoch [6/20]: 100%|█████████████████████████████████████████| 755/755 [00:45<00:00, 16.57it/s, accuracy=tensor(0.5043), loss=1.6]
Epoch [6/20], Loss: 1.5486442747495033, Accuracy: 0.5401603579521179
Epoch [7/20]: 100%|█████████████████████████████████████████| 755/755 [00:45<00:00, 16.56it/s, accuracy=tensor(0.5948), loss=1.4]
Epoch [7/20], Loss: 1.5221538628963445, Accuracy: 0.5444992184638977
Epoch [8/20]: 100%|████████████████████████████████████████| 755/755 [00:50<00:00, 15.06it/s, accuracy=tensor(0.5905), loss=1.32]
Epoch [8/20], Loss: 1.5015746576896567, Accuracy: 0.5478304624557495
Epoch [9/20]: 100%|████████████████████████████████████████| 755/755 [00:53<00:00, 14.09it/s, accuracy=tensor(0.6034), loss=1.31]
Epoch [9/20], Loss: 1.4858304214793325, Accuracy: 0.5502656102180481
Epoch [10/20]: 100%|████████████████████████████████████████| 755/755 [00:55<00:00, 13.63it/s, accuracy=tensor(0.5129), loss=1.6]
Epoch [10/20], Loss: 1.4721728046208817, Accuracy: 0.5521207451820374
Epoch [11/20]:  37%|██████████████▌                        | 282/755 [00:18<00:31, 14.85it/s, accuracy=tensor(0.5065), loss=1.57]
Traceback (most recent call last):
  File "/Users/igorgonteri/Desktop/Nornour/0006-LSTM-peptides/LSTM-peptide-model.py", line 490, in <module>
    model = LSTMPeptides(long_pep)
    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/igorgonteri/Desktop/Nornour/0006-LSTM-peptides/LSTM-peptide-model.py", line 387, in train
    loss, accuracy = model.training_step(x, y, criterion)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/igorgonteri/Desktop/Nornour/0006-LSTM-peptides/LSTM-peptide-model.py", line 333, in training_step
    y_pred, (state_h, state_c) = self(x, (state_h, state_c))
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/LLM2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/LLM2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/igorgonteri/Desktop/Nornour/0006-LSTM-peptides/LSTM-peptide-model.py", line 316, in forward
    output, state = self.lstm(embed, prev_state)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/LLM2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/LLM2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/LLM2/lib/python3.11/site-packages/torch/nn/modules/rnn.py", line 878, in forward
    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
